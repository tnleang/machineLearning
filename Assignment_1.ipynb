{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnleang/machineLearning/blob/master/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "m_R4z-lmbSjo",
        "colab_type": "text"
      },
      "source": [
        "<h2>Assignment 1 - Linear Regression on Boston Housing Data</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "WJ3cJxt3bSjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The modules we're going to use\n",
        "from __future__ import print_function\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets, linear_model\n",
        "from scipy import linalg\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# When you execute a code to plot with a simple SHIFT-ENTER, the plot will be shown directly under the code cell\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDdCIr93goHy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "VXXIgKFJbSjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data from scikit-learn, which returns (data, target)\n",
        "# note: if you call \"boston = load_boston()\", it returns a dictionary-like object\n",
        "data, target = datasets.load_boston(True)\n",
        "\n",
        "# Split the data into two parts: training data and testing data\n",
        "train_data,test_data,train_target,test_target = train_test_split(data,(target[:, np.newaxis]), test_size=0.2, random_state=42)\n",
        "\n",
        "# MSE using numpy\n",
        "def mean_square_err(test_target,predict_target):\n",
        "  return np.square(np.subtract(test_target, predict_target)).sum() /(2 * len(test_target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NA3ezrA-bSjt",
        "colab_type": "text"
      },
      "source": [
        "<h4>Use scikit-learn library in the following cell</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hlqomvfpbSjt",
        "colab_type": "code",
        "outputId": "c99790ba-94c6-49d2-c6a1-da97640a0124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Task 1-1: use linear regression in sklearn\n",
        "regr = linear_model.LinearRegression()\n",
        "regr.fit(train_data,train_target)\n",
        "predict_ptarget = regr.predict(test_data)\n",
        "train_predict = regr.predict(train_data)\n",
        "\n",
        "#print(test_target)\n",
        "#print(predict_ptarget)\n",
        "# Task 1-2: show intercept and coefficents\n",
        "print('Intercept: \\n', regr.intercept_ )\n",
        "print('Coefficients: \\n', regr.coef_)\n",
        "\n",
        "# Task 1-3: show errors on training dataset and testing dataset\n",
        "print(\"Mean squared error train data: %.2f\" %  mean_square_err(train_target,train_predict)) \n",
        "print(\"Mean squared error test data: %.2f\" %  mean_square_err(test_target,predict_ptarget)) "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intercept: \n",
            " [30.24675099]\n",
            "Coefficients: \n",
            " [[-1.13055924e-01  3.01104641e-02  4.03807204e-02  2.78443820e+00\n",
            "  -1.72026334e+01  4.43883520e+00 -6.29636221e-03 -1.44786537e+00\n",
            "   2.62429736e-01 -1.06467863e-02 -9.15456240e-01  1.23513347e-02\n",
            "  -5.08571424e-01]]\n",
            "Mean squared error train data: 10.82\n",
            "Mean squared error test data: 12.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "RpX2tOa5bSjv",
        "colab_type": "text"
      },
      "source": [
        "<h4>Use analytical solution (normal equation) to perform linear regression in the following cell</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "YZJJ0sUGbSjv",
        "colab_type": "code",
        "outputId": "4c28946d-a75b-4cd2-ac69-a4cce105495a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# Task 2-1: Implement a function solving normal equation \n",
        "# Inputs: Training data and  training label\n",
        "# Output: Weights\n",
        "def myNormalEqualFun(X,y):\n",
        "  return np.matmul( np.matmul( linalg.inv( np.matmul(X.T, X)), X.T),y)\n",
        "    \n",
        "# Task 2-2: Implement a function performing prediction\n",
        "# Inputs: Testing data and weights\n",
        "# Output: Predictions\n",
        "def myPredictFun(X,w):\n",
        "    return np.matmul(X, w)\n",
        "\n",
        "# Here we insert a column of 1s into training_data and test_data (to be consistent with our lecture slides)\n",
        "train_data_intercept = np.insert(train_data, 0, 1, axis=1)\n",
        "test_data_intercept = np.insert(test_data, 0, 1, axis=1)\n",
        "\n",
        "# Here we call myNormalEqual to train the model and get weights\n",
        "w = myNormalEqualFun(train_data_intercept,train_target)\n",
        "predict_target = myPredictFun(test_data_intercept, w)\n",
        "train_predict = myPredictFun(train_data_intercept, w)\n",
        "\n",
        "\n",
        "# Task 2-3: show intercept and coefficents\n",
        "print('Intercept: \\n', w[0] )\n",
        "print('Coefficents: \\n',w[1:])\n",
        "\n",
        "\n",
        "\n",
        "# Task 2-4: show errors on training dataset and testing dataset\n",
        "print(\"Mean squared error train data: %.2f\" %  mean_square_err(train_target,train_predict)) \n",
        "print(\"Mean squared error test data: %.2f\" %  mean_square_err(test_target,predict_target)) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intercept: \n",
            " [30.24675099]\n",
            "Coefficents: \n",
            " [[-1.13055924e-01]\n",
            " [ 3.01104641e-02]\n",
            " [ 4.03807204e-02]\n",
            " [ 2.78443820e+00]\n",
            " [-1.72026334e+01]\n",
            " [ 4.43883520e+00]\n",
            " [-6.29636221e-03]\n",
            " [-1.44786537e+00]\n",
            " [ 2.62429736e-01]\n",
            " [-1.06467863e-02]\n",
            " [-9.15456240e-01]\n",
            " [ 1.23513347e-02]\n",
            " [-5.08571424e-01]]\n",
            "Mean squared error train data: 10.82\n",
            "Mean squared error test data: 12.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1hhVEfixbSjx",
        "colab_type": "text"
      },
      "source": [
        "<h4>Use numerical solution (baisc gradient descent) to perform linear regression in the following cell</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3H1IxOBubSjy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "c046734c-582b-4fb8-9302-59ce946aecae"
      },
      "source": [
        "# Feature scaling\n",
        "scaler = preprocessing.StandardScaler().fit(train_data)\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "# compute loss\n",
        "def lossFunction(X,y):\n",
        "  return (1/(2*len(y)))*np.sum((X-y)**2)\n",
        "  \n",
        "\n",
        "\n",
        "# Task 3-1: Implement a function performing gradient descent\n",
        "# Inputs: Training data, training label, leaerning rate, number of iterations\n",
        "# Output: the final Weights\n",
        "#         the loss history along iterations\n",
        "def myGradientDescentFun(X,y,learning_rate,numItrs):\n",
        "    # split the train data into train and validation data\n",
        "    # train_X, valid_X, train_y, valid_y = train_test_split(data,(target[:, np.newaxis]), test_size=0.2, random_state=42)\n",
        "    \n",
        "    n_size = len(y)\n",
        "    w = np.random.rand(np.size(X,1),1)\n",
        "    history = []\n",
        "    history_v = []\n",
        "    #history_w = np.zeros((numItrs))\n",
        "    best_w = w\n",
        "    \n",
        "    for i in range(numItrs):\n",
        "      gradient = -(1 / n_size) * ((X.T @ (y - (X @ w))))\n",
        "      w = w - learning_rate * gradient\n",
        "      y_predict = myPredictFun(X,w)\n",
        "      y_validate = myPredictFun(X,w)\n",
        "      history.append(lossFunction(y, y_predict))\n",
        "      # history_v.append(lossFunction(valid_y @ w, y_validate))\n",
        "      \n",
        "    return w,history,history_v\n",
        "\n",
        "# Task 3-2: Implement a function performing prediction\n",
        "# Inputs: Testing data and weights\n",
        "# Output: Predictions\n",
        "def myPredictFun(X,w):\n",
        "    return np.matmul(X, w)\n",
        "\n",
        "# Here we insert a column of 1s into training_data and test_data (to be consistent with our lecture slides)\n",
        "train_data_intercept = np.insert(train_data, 0, 1, axis=1)\n",
        "test_data_intercept = np.insert(test_data, 0, 1, axis=1)\n",
        "\n",
        "# Here we call myGradientDescentFun to train the model and get weights\n",
        "# Note: you need to figure out good learning rate value and the number of iterations\n",
        "w, loss, loss_v = myGradientDescentFun(train_data_intercept,train_target,0.1,20000)\n",
        "\n",
        "predict_target = myPredictFun(test_data_intercept, w)\n",
        "train_predict = myPredictFun(train_data_intercept, w)\n",
        "print(w)\n",
        "#print(predict_target)\n",
        "#print(w)\n",
        "\n",
        "#predict_target = myPredictFun(test_data_intercept, w)\n",
        "\n",
        "# Task 3-3: show intercept and coefficents\n",
        "\n",
        "\n",
        "# Task 3-4: show errors on training dataset and testing dataset\n",
        "print(\"Mean squared error train data: %.2f\" %  mean_square_err(train_target,train_predict)) \n",
        "print(\"Mean squared error test data: %.2f\" %  mean_square_err(test_target,predict_target)) \n",
        "\n",
        "\n",
        "# Task 3-5: plot learning curves showing training errors and testing errors along iterations\n",
        "\n",
        "# # Get training and test loss histories\n",
        "# training_loss = loss.history['loss']\n",
        "# # Create count of the number of epochs\n",
        "# epoch_count = range(1, len(training_loss) + 1)\n",
        "# # Visualize loss history\n",
        "# plt.plot(epoch_count, training_loss, 'r--')\n",
        "# plt.legend(['Training Loss'])\n",
        "# plt.xlabel('Iterations')\n",
        "# plt.ylabel('Loss')\n",
        "plt.plot(loss)\n",
        "plt.plot(loss_v)\n",
        "plt.show()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[22.79653465]\n",
            " [-1.00213533]\n",
            " [ 0.69626862]\n",
            " [ 0.27806485]\n",
            " [ 0.7187384 ]\n",
            " [-2.0223194 ]\n",
            " [ 3.14523956]\n",
            " [-0.17604788]\n",
            " [-3.0819076 ]\n",
            " [ 2.25140666]\n",
            " [-1.76701378]\n",
            " [-2.03775151]\n",
            " [ 1.12956831]\n",
            " [-3.61165842]]\n",
            "Mean squared error train data: 10.82\n",
            "Mean squared error test data: 12.15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx1UOqmfcNe0",
        "colab_type": "text"
      },
      "source": [
        "<h4>Use numerical solution (stochastic gradient descent) to perform linear regression in the following cell</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96_WVyVKcNss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature scaling\n",
        "scaler = preprocessing.StandardScaler().fit(train_data)\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "\n",
        "# Task 4-1: Implement a function performing gradient descent\n",
        "# Inputs: Training data, training label, leaerning rate, number of epoches, batch size\n",
        "# Output: the final Weights\n",
        "#         the loss history along batches\n",
        "def myGradientDescentFun(X,y,learning_rate,epoches, batchsize):\n",
        "  a = np.range(X.shape[0]) # number of rows in X    \n",
        "  np. random.shuffle(a)\n",
        "  new_X = X(a)\n",
        "  new_y = y(a)\n",
        "  \n",
        "  X_batches = []\n",
        "  y_batches = []\n",
        "  for i in range(0, Xshape[0], batchsize):\n",
        "    X_batch = X[i:i+batchsize]\n",
        "    y_batch = y[i:i+batchsize]\n",
        "    X_batches.append(X_batch)\n",
        "    y_batches.append(y_batch)\n",
        "  return X_batches, y_batches\n",
        "\n",
        "# Task 4-2: Implement a function performing prediction\n",
        "# Inputs: Testing data and weights\n",
        "# Output: Predictions\n",
        "def myPredictFun(X,w):\n",
        "    return\n",
        "\n",
        "# Here we insert a column of 1s into training_data and test_data (to be consistent with our lecture slides)\n",
        "train_data_intercept = np.insert(train_data, 0, 1, axis=1)\n",
        "test_data_intercept = np.insert(test_data, 0, 1, axis=1)\n",
        "\n",
        "# Here we call myGradientDescentFun to train the model and get weights\n",
        "# Note: you need to figure out good learning rate value and the number of iterations\n",
        "w, loss = myGradientDescentFun(train_data_intercept,train_target,0,0,0)\n",
        "\n",
        "# Task 4-3: show intercept and coefficents\n",
        "\n",
        "\n",
        "# Task 4-4: show errors on training dataset and testing dataset\n",
        "\n",
        "\n",
        "# Task 4-5: plot learning curves showing training errors and testing errors along bath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUwBA38_lG3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_gradient(X,y,batchsize):\n",
        "  # two ways to shuffle\n",
        "  #First\n",
        "  z = np.hstack(X,y)\n",
        "  np.random.shuffle(z)\n",
        "  \n",
        "  # Second\n",
        "  a = np.range(X.shape[0]) # number of rows in X\n",
        "  np.random.shuffle(a)\n",
        "  new_X = X(a)\n",
        "  new_y = y(a)\n",
        "  \n",
        "  X_batches = []\n",
        "  y_batches = []\n",
        "  for i in range(0,X.shape[0],batchsize):\n",
        "    X_batch = X[i:i+batchsize]\n",
        "    y_batch = y[i:i+batchsize]\n",
        "    X_batches.append(X_batch)\n",
        "    y_batches.append(y_bathc)\n",
        "  return X_batches, y_batches\n",
        "\n",
        "\n",
        "def batch_gradient2(X,y,batchsize):\n",
        "  \n",
        "  \n",
        "  for i in range(0,X.shape[0],batchsize):\n",
        "    yield (X[i:i+batchsize], y[i:i+batchsize])\n",
        "    \n",
        "for batch in batch_gradient2(....):\n",
        "  \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}